# H∆∞·ªõng D·∫´n T√≠ch H·ª£p M√¥ H√¨nh AI ü§ñ

## üìã T·ªïng Quan
File n√†y h∆∞·ªõng d·∫´n c√°ch t√≠ch h·ª£p m√¥ h√¨nh AI th·ª±c t·∫ø c·ªßa b·∫°n v√†o ·ª©ng d·ª•ng Robot AI ƒë·ªÉ ph√°t hi·ªán con ng∆∞·ªùi.

## üîß C·∫•u Tr√∫c Hi·ªán T·∫°i

### AIModelService
File: `lib/services/ai_model_service.dart`

ƒê√¢y l√† service ch√≠nh ƒë·ªÉ x·ª≠ l√Ω m√¥ h√¨nh AI v·ªõi c√°c ph∆∞∆°ng th·ª©c:

```dart
class AIModelService {
  // Singleton pattern
  static final AIModelService _instance = AIModelService._internal();
  factory AIModelService() => _instance;
  
  // Kh·ªüi t·∫°o m√¥ h√¨nh
  Future<bool> initializeModel();
  
  // X·ª≠ l√Ω h√¨nh ·∫£nh
  Future<String?> processImage(CameraImage cameraImage);
  
  // Gi·∫£i ph√≥ng t√†i nguy√™n
  void dispose();
}
```

## üöÄ C√°ch T√≠ch H·ª£p M√¥ H√¨nh Th·ª±c T·∫ø

### B∆∞·ªõc 1: Th√™m Dependencies
Trong `pubspec.yaml`, th√™m dependencies cho m√¥ h√¨nh AI:

```yaml
dependencies:
  # Cho TensorFlow Lite
  tflite_flutter: ^0.10.4
  tflite_flutter_helper: ^0.3.1
  
  # Cho ML Kit (Google)
  google_ml_kit: ^0.16.3
  
  # Cho custom model loading
  flutter/services: # ƒë·ªÉ load assets
```

### B∆∞·ªõc 2: Th√™m Model v√†o Assets
Trong `pubspec.yaml`:

```yaml
flutter:
  assets:
    - assets/models/your_model.tflite
    - assets/models/labels.txt
```

### B∆∞·ªõc 3: C·∫≠p Nh·∫≠t initializeModel()

Thay th·∫ø ph·∫ßn simulation trong `initializeModel()`:

```dart
Future<bool> initializeModel() async {
  try {
    debugPrint("ƒêang kh·ªüi t·∫°o m√¥ h√¨nh AI...");
    
    // V√≠ d·ª• v·ªõi TensorFlow Lite
    _interpreter = await Interpreter.fromAsset('assets/models/your_model.tflite');
    
    // C·∫•u h√¨nh input/output tensors
    _inputShape = _interpreter.getInputTensor(0).shape;
    _outputShape = _interpreter.getOutputTensor(0).shape;
    
    // Load labels n·∫øu c·∫ßn
    _labels = await _loadLabels('assets/models/labels.txt');
    
    _isModelLoaded = true;
    debugPrint("M√¥ h√¨nh AI ƒë√£ s·∫µn s√†ng!");
    return true;
  } catch (e) {
    debugPrint("L·ªói kh·ªüi t·∫°o m√¥ h√¨nh: $e");
    return false;
  }
}
```

### B∆∞·ªõc 4: C·∫≠p Nh·∫≠t _runInference()

Thay th·∫ø ph·∫ßn simulation trong `_runInference()`:

```dart
Future<String> _runInference(Uint8List imageData) async {
  try {
    // Preprocess image
    final input = _preprocessImage(imageData);
    
    // Run inference
    final output = List.filled(_outputShape.reduce((a, b) => a * b), 0.0);
    _interpreter.run(input, output);
    
    // Postprocess results
    final result = _postprocessOutput(output);
    
    debugPrint("K·∫øt qu·∫£ t·ª´ m√¥ h√¨nh: $result");
    return result;
    
  } catch (e) {
    debugPrint("L·ªói trong qu√° tr√¨nh inference: $e");
    return "error";
  }
}
```

## üõ†Ô∏è V√≠ D·ª• T√≠ch H·ª£p C·ª• Th·ªÉ

### V·ªõi TensorFlow Lite

```dart
import 'package:tflite_flutter/tflite_flutter.dart';

class AIModelService {
  Interpreter? _interpreter;
  List<String>? _labels;
  
  Future<bool> initializeModel() async {
    try {
      _interpreter = await Interpreter.fromAsset('assets/models/human_detection.tflite');
      _labels = await _loadLabels('assets/models/labels.txt');
      _isModelLoaded = true;
      return true;
    } catch (e) {
      return false;
    }
  }
  
  Future<String> _runInference(Uint8List imageData) async {
    if (_interpreter == null) return "error";
    
    // Resize image to model input size (e.g., 224x224)
    final input = _resizeImage(imageData, 224, 224);
    
    // Normalize pixel values
    final normalizedInput = _normalizeImage(input);
    
    // Run inference
    final output = List.filled(2, 0.0); // [no_human, human]
    _interpreter!.run([normalizedInput], [output]);
    
    // Interpret results
    final confidence = output[1]; // human confidence
    if (confidence > 0.7) {
      return "human_detected";
    } else if (confidence > 0.3) {
      return "human_far";
    } else {
      return "no_human";
    }
  }
}
```

### V·ªõi Google ML Kit

```dart
import 'package:google_ml_kit/google_ml_kit.dart';

class AIModelService {
  PersonDetector? _personDetector;
  
  Future<bool> initializeModel() async {
    try {
      _personDetector = GoogleMlKit.vision.personDetector(
        PersonDetectorOptions(
          mode: PersonDetectorMode.stream,
          enablePoseLandmarks: false,
        ),
      );
      _isModelLoaded = true;
      return true;
    } catch (e) {
      return false;
    }
  }
  
  Future<String?> processImage(CameraImage cameraImage) async {
    if (_personDetector == null) return null;
    
    try {
      final inputImage = _convertToInputImage(cameraImage);
      final persons = await _personDetector!.processImage(inputImage);
      
      if (persons.isEmpty) {
        return "no_human";
      } else if (persons.length == 1) {
        final person = persons.first;
        final size = person.boundingBox.size;
        if (size.width * size.height > 50000) {
          return "human_close";
        } else {
          return "human_far";
        }
      } else {
        return "multiple_humans";
      }
    } catch (e) {
      return "error";
    }
  }
}
```

## üìù C√°c B∆∞·ªõc Ti·∫øp Theo

### 1. X√≥a Simulation Code
Trong `follow_human_screen.dart`, thay th·∫ø:
```dart
final result = await _simulateAIProcessing();
```

B·∫±ng:
```dart
final result = await _aiService.processImage(cameraImage);
```

### 2. C·∫•u H√¨nh Camera Stream
ƒê·ªÉ c√≥ hi·ªáu su·∫•t t·ªët h∆°n, s·ª≠ d·ª•ng camera stream thay v√¨ takePicture():

```dart
_cameraController!.startImageStream((CameraImage image) async {
  if (!_isProcessing) {
    _isProcessing = true;
    final result = await _aiService.processImage(image);
    if (result != null && mounted) {
      _handleDetectionResult(result);
    }
    _isProcessing = false;
  }
});
```

### 3. T·ªëi ∆Øu Hi·ªáu Su·∫•t
- Gi·∫£m resolution camera n·∫øu c·∫ßn
- TƒÉng interval gi·ªØa c√°c l·∫ßn x·ª≠ l√Ω
- S·ª≠ d·ª•ng isolate cho heavy processing

## üîÑ Test v√† Debug

### 1. Logging
Th√™m c√°c log chi ti·∫øt:
```dart
debugPrint("Model input shape: $_inputShape");
debugPrint("Processing time: ${stopwatch.elapsedMilliseconds}ms");
debugPrint("Confidence scores: $confidenceScores");
```

### 2. Error Handling
```dart
try {
  final result = await _aiService.processImage(image);
} catch (e) {
  debugPrint("AI processing error: $e");
  // Fallback to previous detection or default state
}
```

## üìä K·∫øt Qu·∫£ Mong ƒê·ª£i

Sau khi t√≠ch h·ª£p th√†nh c√¥ng, b·∫°n s·∫Ω c√≥:

‚úÖ Real-time human detection t·ª´ camera  
‚úÖ Ph√¢n lo·∫°i: no_human, human_detected, human_far, human_close, multiple_humans  
‚úÖ UI feedback t∆∞∆°ng ·ª©ng v·ªõi k·∫øt qu·∫£ detection  
‚úÖ Smooth animations v√† status updates  
‚úÖ Optimal performance cho mobile device  

## üéØ Tips v√† Best Practices

1. **Model Size**: Gi·ªØ model < 10MB cho mobile
2. **Inference Time**: M·ª•c ti√™u < 100ms per frame
3. **Memory Usage**: Monitor memory usage v·ªõi profiler
4. **Battery**: Optimize ƒë·ªÉ ti·∫øt ki·ªám pin
5. **Error Handling**: Lu√¥n c√≥ fallback strategy

Ch√∫c b·∫°n t√≠ch h·ª£p th√†nh c√¥ng! üöÄ 